{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with ID have been saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer  # Required to enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import joblib  # Import joblib for loading the model and preprocessing objects\n",
    "test_file_path=\n",
    "# Step 1: Load the new test data (numerical features only)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# Step 2: Extract the 'ID' column and store it separately\n",
    "ids = test_data['ID']\n",
    "\n",
    "# Step 3: Drop unnecessary columns, including 'ID', same as done during training\n",
    "columns_to_drop = ['ID', 'Column9', 'Column5', 'Column10', 'Column11', 'Column13', \n",
    "                   'Column14', 'Column15', 'Column16', 'Column20', 'Column21']\n",
    "test_data = test_data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Step 4: Imputation of missing values using IterativeImputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "# Impute only numerical columns\n",
    "numerical_columns = test_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "test_data_imputed = pd.DataFrame(imputer.fit_transform(test_data[numerical_columns]), columns=numerical_columns)\n",
    "\n",
    "# Combine back non-numeric columns if any (though your dataset is numerical, in case future use has other columns)\n",
    "test_data[test_data.select_dtypes(exclude=['float64', 'int64']).columns] = test_data.select_dtypes(exclude=['float64', 'int64'])\n",
    "\n",
    "# Replace imputed data with the original test data\n",
    "test_data.update(test_data_imputed)\n",
    "\n",
    "# Step 5: Load the scaler and trained model\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "model = joblib.load('catboost_model.pkl')  # Replace with your actual path\n",
    "\n",
    "# Step 6: Apply scaling to the test data\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "\n",
    "# Step 7: Predict using the trained model\n",
    "y_pred_new = model.predict(scaled_test_data)\n",
    "\n",
    "# Step 8: Combine the 'ID' column with the predictions\n",
    "predictions_df = pd.DataFrame({'ID': ids, 'target': y_pred_new})\n",
    "\n",
    "# Step 9: Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions with ID have been saved to predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
